{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2123b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from maddpg import MADDPG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05625147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c92a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "maddpg = MADDPG(state_size=24, action_size=2, num_agents=2, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525da7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mono path[0] = '/Users/cassimiro/code/projects/Multi-Agent-Tennis-Unity/RL-Multi-Agent-Tennis-Unity/Tennis.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/cassimiro/code/projects/Multi-Agent-Tennis-Unity/RL-Multi-Agent-Tennis-Unity/Tennis.app/Contents/MonoBleedingEdge/etc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "\n",
    "# Get the brain name and brain from the environment\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9b6350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Score: 2.600000038743019\n"
     ]
    }
   ],
   "source": [
    "def play(n_episodes=10):\n",
    "    # Load the saved agent weights\n",
    "    for idx, agent in enumerate(maddpg.agents):\n",
    "        agent.actor_local.load_state_dict(torch.load(f'agent{idx}_actor.pth'))\n",
    "        agent.critic_local.load_state_dict(torch.load(f'critic{idx}_critic.pth'))\n",
    "\n",
    "    # Iterate through the episodes\n",
    "    for i_episode in range(n_episodes):\n",
    "        # Reset the environment for each episode\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        scores = np.zeros(2)\n",
    "\n",
    "        for _ in range(1000):\n",
    "            # Get actions for the current state from the MADDPG model without adding noise\n",
    "            action = maddpg.act(state, i_episode, add_noise=False)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "\n",
    "            # Obtain the next state, reward, and done flag from the environment\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "\n",
    "            # Update the scores\n",
    "            scores += reward\n",
    "\n",
    "            # Terminate the episode if any agent is done\n",
    "            if np.any(done):\n",
    "                break\n",
    "\n",
    "            # Update the current state\n",
    "            state = next_state\n",
    "\n",
    "        print(f'Episode {i_episode + 1}, Score: {np.max(scores)}')\n",
    "\n",
    "# Call the play function to see the performance of the trained agents\n",
    "play(n_episodes=1)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576abcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
